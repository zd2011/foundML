{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning (CSCI-UA.473)\n",
    "\n",
    "## Lab 2: Regression, Loss Functions and Regularization\n",
    "\n",
    "### Goal:  Demonstrate some other facets of Regression, different loss functions and Regularization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import some packages we'll need.\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "matplotlib.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data:\n",
    "data = np.genfromtxt('housingsata.csv', delimiter=',')\n",
    "print(data.shape)\n",
    "\n",
    "X, y = data[:,:4], data[:,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets try understanding the least squares regression from a linear algebra (geometric) perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before we go into the details of this, recall the principles of vector projection from the lecture:\n",
    "vec1 = np.array([1,0.25]) #Vector 1\n",
    "vec2 = np.array([1.5,2]) #Vector 2\n",
    "magVec1 = np.sqrt(vec1[0]**2 + vec1[1]**2) #Magnitude of vector 1\n",
    "magVec2 = np.sqrt(vec2[0]**2 + vec2[1]**2) #Magnitude of vector 2 \n",
    "dotProduct = np.dot(vec1,vec2)  # Using a function to get the dot product \n",
    "angleBetween = np.degrees(np.arccos(dotProduct/(magVec1*magVec2))) #What is the angle between the vectors?\n",
    "uVec = vec2/magVec2 # Creating a unit vector out of vec2 by dividing by magnitude\n",
    "\n",
    "p = magVec1 * np.cos(np.deg2rad(angleBetween)) # The projection direction\n",
    "projVec = p * uVec # That's the actual projected vector, yielded by p multiplied with the unit vector\n",
    "projVec = vec2 *np.dot(vec1, vec2)/np.dot(vec2, vec2)\n",
    "plt.plot([0,vec1[0]],[0,vec1[1]],color='green',linewidth=2) # Plot vec1 in purple\n",
    "plt.plot([0,uVec[0]],[0,uVec[1]],color='blue',linewidth=2) # Plot uVec in blue\n",
    "plt.plot([0,projVec[0]],[0,projVec[1]],color='red',linewidth=2) # Plot the projection of vec1 onto vec2 in red\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.axis('equal'); #Make sure aspect ratio is the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets use the geometric interpretation seen in the lectures to our housing dataset\n",
    "# To make things easily visualised we will use only a subset of our dataset\n",
    "X_sub = X[:3, 0]\n",
    "y_sub = y[:3]/1e3\n",
    "\n",
    "fig = plt.figure() # init figure\n",
    "ax = fig.gca(projection='3d') # project into 3d space\n",
    "\n",
    "ax.plot3D([0,X_sub[0]],[0,X_sub[1]],[0,X_sub[2]],color='blue',linewidth=2) \n",
    "ax.plot3D([0,y_sub[0]],[0,y_sub[1]],[0,y_sub[2]],color='green',linewidth=2) \n",
    "plt.legend(['Size (in sq ft)','Price (in \\$)']) \n",
    "ax.set_xlabel('House 1') \n",
    "ax.set_ylabel('House 2') \n",
    "ax.set_zlabel('House 3')\n",
    "print(X_sub, y_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's actually use the formula we derived\n",
    "# We use the projection formula to find beta to minimize the distance\n",
    "# between beta*input and output. Output = beta*input + error\n",
    "\n",
    "beta = np.dot(y_sub,X_sub)/np.dot(X_sub,X_sub) # Find the beta\n",
    "prediction = beta * X_sub # Make a prediction (simplest possible)\n",
    "print(prediction)\n",
    "model = LinearRegression().fit(X_sub.reshape(1,-1), y_sub.reshape(1,-1))\n",
    "model.predict(X_sub.reshape(1,-1))\n",
    "print(prediction)\n",
    "# Add this to the plot - the plot thickens:\n",
    "fig = plt.figure() # init figure\n",
    "ax = fig.gca(projection='3d') # project into 3d space\n",
    "ax.plot3D([0,X_sub[0]],[0,X_sub[1]],[0,X_sub[2]],color='blue',linewidth=2) \n",
    "ax.plot3D([0,y_sub[0]],[0,y_sub[1]],[0,y_sub[2]],color='green',linewidth=2) \n",
    "ax.plot3D([0,prediction[0]],[0,prediction[1]],[0,prediction[2]],color='lime',linewidth=2,linestyle='dotted') \n",
    "plt.legend(['Size (in sq ft)','Price (in \\$)','Prediction']) \n",
    "ax.set_xlabel('House 1') \n",
    "ax.set_ylabel('House 2') \n",
    "ax.set_zlabel('House 3') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Let's explicitly add the distance between the two (prediction and outcome)\n",
    "\n",
    "fig = plt.figure() # init figure\n",
    "ax = fig.gca(projection='3d') # project into 3d space\n",
    "ax.plot3D([0, X_sub[0]],[0,X_sub[1]],[0,X_sub[2]],color='blue',linewidth=2) \n",
    "ax.plot3D([0,y_sub[0]],[0,y_sub[1]],[0,y_sub[2]],color='green',linewidth=2) \n",
    "ax.plot3D([0,prediction[0]],[0,prediction[1]],[0,prediction[2]],color='lime',linewidth=2,linestyle='dotted')\n",
    "ax.plot3D([y_sub[0],prediction[0]],[y_sub[1],prediction[1]],[y_sub[2],prediction[2]],color='red',linewidth=2)  \n",
    "plt.legend(['Size (in sq ft)','Price','Prediction','Error']) \n",
    "ax.set_xlabel('House 1') \n",
    "ax.set_ylabel('House 2') \n",
    "ax.set_zlabel('House 3') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Now that we convinced ourselves that this is in fact the correct beta (geometrically)\n",
    "# we can go back and plot the solution\n",
    "# We could open the old figure again, but let's start from scratch\n",
    "# What if we had 30 houses?\n",
    "maxArea = 4000\n",
    "X_sub = X[:30, 0] \n",
    "y_sub = y[:30]/1e3\n",
    "beta = np.dot(y_sub,X_sub)/np.dot(X_sub,X_sub) # Find the beta\n",
    "prediction = beta * X_sub # Make a prediction (simplest possible)\n",
    "\n",
    "regressionLineX = np.linspace(0,maxArea,10) # Gives us 10 equally spaced numbers between 0 and 4000. Intrapolation, x-base\n",
    "regressionLineY = beta * regressionLineX # Find the ys of the regression line\n",
    "plt.plot(X_sub,y_sub,'o',markersize=5) # Plot the data\n",
    "plt.plot(regressionLineX,regressionLineY,color='black') # Plot regression line\n",
    "plt.plot([X_sub,X_sub],[prediction,y_sub],color='red') # Residuals\n",
    "plt.xlabel('Area (in sq ft)')\n",
    "plt.ylabel('Price (x 1e3)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But least squares is not our only option...infact it is not always the best one either!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startExploration = beta - 2\n",
    "endExploration = beta + 2\n",
    "numBeta = 200\n",
    "testBetas = np.linspace(startExploration,endExploration,numBeta)\n",
    "distanceSum = np.empty([numBeta,4]) # Init container\n",
    "distanceSum[:] = np.NaN # Convert to NaN\n",
    "\n",
    "for ii in range(numBeta):\n",
    "    prediction = testBetas[ii] * X_sub\n",
    "    distanceSum[ii,0] = sum(prediction-y_sub) # Simple\n",
    "    distanceSum[ii,1] = sum((prediction-y_sub)**2) # Sum of squares\n",
    "    distanceSum[ii,2] = sum(abs(prediction-y_sub)) # Absolute value\n",
    "    distanceSum[ii,3] = sum(np.log(1 + (prediction-y_sub)**2)) # Lorentzian\n",
    "\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "for ii in range(int(np.size(distanceSum)/len(distanceSum))):\n",
    "    ax = fig.add_subplot(2, 2, ii+1)\n",
    "#     plt.subplot(2,2,ii+1)\n",
    "    ax.plot(testBetas,distanceSum[:,ii])\n",
    "    ax.set_xlabel(r\"$\\beta$ values\")\n",
    "    ax.set_ylabel(\"Distance\")\n",
    "    if ii == 0:\n",
    "        ax.set_title('Summed differences')\n",
    "    elif ii == 1:\n",
    "        ax.set_title('Sum of square differences')\n",
    "    elif ii == 2:\n",
    "        ax.set_title('Sum of absolute differences')\n",
    "    else:\n",
    "        ax.set_title('Lorentzian')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To see how least squares might be completely inappropriate for some situations consider the following synthetic data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Init data:\n",
    "x1 = np.array([10,8,13,9,11,14,6,4,12,7,5])\n",
    "x2 = np.copy(x1)\n",
    "x3 = np.copy(x1)\n",
    "x4 = np.array([8,8,8,8,8,8,8,19,8,8,8])\n",
    "y1 = np.array([8.04,6.95,7.58,8.81,8.33,9.96,7.24,4.26,10.84,4.82,5.68])\n",
    "y2 = np.array([9.14,8.14,8.74,8.77,9.26,8.1,6.13,3.1,9.13,7.26,4.74])\n",
    "y3 = np.array([7.46,6.77,12.74,7.11,7.81,8.84,6.08,5.39,8.15,6.42,5.73])\n",
    "y4 = np.array([6.58,5.76,7.71,8.84,8.47,7.04,5.25,12.5,5.56,7.91,6.89])\n",
    "\n",
    "# Plot data:\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(x1,y1,'o')\n",
    "model = LinearRegression().fit(x1.reshape(-1,1),y1.reshape(-1,1))\n",
    "xVals = np.linspace(4,14,101)\n",
    "regressLine = model.coef_ * xVals + model.intercept_\n",
    "rSqr = model.score(x1.reshape(-1,1),y1.reshape(-1,1))\n",
    "plt.plot(xVals,regressLine.flatten())\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('y1')\n",
    "plt.title('B0 = {:.0f}'.format(model.intercept_[0]) + ', B1 = {:.1f}'.format(model.coef_[0][0]) + ', R^2 = {:.2f}'.format(rSqr))\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(x2,y2,'o')\n",
    "model = LinearRegression().fit(x2.reshape(-1,1),y2.reshape(-1,1))\n",
    "xVals = np.linspace(4,14,101)\n",
    "regressLine = model.coef_ * xVals + model.intercept_\n",
    "rSqr = model.score(x2.reshape(-1,1),y2.reshape(-1,1))\n",
    "plt.plot(xVals,regressLine.flatten())\n",
    "plt.xlabel('x2')\n",
    "plt.ylabel('y2')\n",
    "plt.title('B0 = {:.0f}'.format(model.intercept_[0]) + ', B1 = {:.1f}'.format(model.coef_[0][0]) + ', R^2 = {:.2f}'.format(0.67))\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.plot(x3,y3,'o')\n",
    "model = LinearRegression().fit(x3.reshape(-1,1),y3.reshape(-1,1))\n",
    "xVals = np.linspace(4,14,101)\n",
    "regressLine = model.coef_ * xVals + model.intercept_\n",
    "rSqr = model.score(x3.reshape(-1,1),y3.reshape(-1,1))\n",
    "plt.plot(xVals,regressLine.flatten())\n",
    "plt.xlabel('x3')\n",
    "plt.ylabel('y3')\n",
    "plt.title('B0 = {:.0f}'.format(model.intercept_[0]) + ', B1 = {:.1f}'.format(model.coef_[0][0]) + ', R^2 = {:.2f}'.format(0.67))\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.plot(x4,y4,'o')\n",
    "model = LinearRegression().fit(x4.reshape(-1,1),y4.reshape(-1,1))\n",
    "xVals = np.linspace(8,19,101)\n",
    "regressLine = model.coef_ * xVals + model.intercept_\n",
    "rSqr = model.score(x4.reshape(-1,1),y4.reshape(-1,1))\n",
    "plt.plot(xVals,regressLine.flatten())\n",
    "plt.xlabel('x4')\n",
    "plt.ylabel('y4')\n",
    "plt.title('B0 = {:.0f}'.format(model.intercept_[0]) + ', B1 = {:.1f}'.format(model.coef_[0][0]) + ', R^2 = {:.2f}'.format(0.67))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression and Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters:\n",
    "noiseMagnitude = 2 # how much random noise is there?\n",
    "numData = 8 # how many measurements (samples) of the signal?\n",
    "numPoints = 1001 \n",
    "leftRange = -5 \n",
    "rightRange = 5\n",
    "x = np.linspace(leftRange,rightRange,numPoints) # determine the location \n",
    "# of evenly spaced points from -5 to 5 to use as an x-base\n",
    "\n",
    "# Determine the functional relationship between x and y in reality (ground truth):\n",
    "sig = 1 # user determines whether the signal is quadratic (1) or cubic (2)\n",
    "if sig == 1:\n",
    "    y1 = x**2 # quadratic function\n",
    "elif sig == 2:\n",
    "    y1 = x**3 # cubic function\n",
    "    \n",
    "# Compute signal plus noise:\n",
    "y = y1 + noiseMagnitude * np.random.normal(0,1,numPoints) # signal + noise\n",
    "\n",
    "# Plot data:\n",
    "plt.figure(1)\n",
    "plt.plot(x,y1,color='blue',linewidth=5)\n",
    "plt.xlabel('X') \n",
    "plt.ylabel('Y')  \n",
    "plt.title('Ground Truth')\n",
    "\n",
    "plt.figure(2)\n",
    "plt.plot(x,y,color='blue',linewidth=1)\n",
    "plt.xlabel('X') \n",
    "plt.ylabel('Y')  \n",
    "plt.title('Signal plus noise')\n",
    "\n",
    "#Ground truth with noise in one plot\n",
    "plt.figure(3)\n",
    "plt.plot(x,y,color='blue',linewidth=1)\n",
    "plt.plot(x,y1,color='black',linewidth=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Determine the location of the sampling (measuring) points \n",
    "\n",
    "# Randomly draw points to sample:\n",
    "samplingIndices = np.random.randint(1,numPoints,numData) # random points, from anywhere on the signal\n",
    "\n",
    "# Plot data as a subsample of the noisy signal:\n",
    "plt.plot(x,y,color='blue',linewidth=1)\n",
    "plt.plot(x,y1,color='black',linewidth=5)\n",
    "plt.plot(x[samplingIndices],y[samplingIndices],'o',markersize=4,color='red')\n",
    "plt.xlim(-5,5) # keep it on the same x-range as before\n",
    "\n",
    "# Note: Parabola doesn't fit perfectly because there is noise (measurement error). We are\n",
    "# overfitting to noise. The more noise, the worse this effect is\n",
    "# In real life, all measurements are contaminated with noise, so overfitting\n",
    "# to noise is always a concern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% (Over)fitting successive polynomials and calculating RMSE at each point\n",
    "\n",
    "rmse = np.array([]) # capture RMSE for each polynomial degree\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "for ii in range(numData): # loop through each sampling point\n",
    "    ax = fig.add_subplot(2,4,ii+1)\n",
    "    numDegrees = ii+1 # degree of polynomial to fit to our 8 data points\n",
    "    p = np.polyfit(x[samplingIndices],y[samplingIndices],numDegrees) # returns a vector of coefficients p that minimizes the squared error\n",
    "    yHat = np.polyval(p,x) # evaluate the polynomial at specific values\n",
    "    ax.plot(x,yHat,color='blue',linewidth=1)\n",
    "    ax.plot(x[samplingIndices],y[samplingIndices],'ro',markersize=3)\n",
    "    error = np.sqrt(np.mean((y[samplingIndices] - yHat[samplingIndices])**2))\n",
    "    ax.set_title('Degrees: {}'.format(numDegrees) + ', RMSE = {:.3f}'.format(error))\n",
    "    rmse = np.append(rmse,error) # keep track of RMSE - we will use this later\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y (Signal + Noise)')\n",
    "    \n",
    "fig.suptitle('Fits for different degrees of polynomials')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%% Plotting RMSE of the training set as function of polynomial degree\n",
    "plt.plot(np.linspace(1,numData,numData),rmse)\n",
    "plt.xlabel('Degree of polynomial')\n",
    "plt.ylabel('RMSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Leave-one-out procedure to cross-validate the number of terms in the\n",
    "# model. Note: We randomly pick one of the test points to use to calculate\n",
    "# the RMSE with. We use the other data points to fit the model\n",
    "# This method is called \"leave one out\" and is very computationally\n",
    "# expensive, as one has to fit the model n-1 times\n",
    "\n",
    "# Initialize parameters:\n",
    "numRepeats = 100 # Number of samples - how often are we doing this?\n",
    "rmse = np.zeros([numRepeats,numData-1]) # Reinitialize RMSE (100x7)\n",
    "# For each polynomial degree, 100x we are going to randomly pick one of\n",
    "# the points from the set of 8 and compute the RMSE\n",
    "# We are then going to fit the model from the remaining (7) points\n",
    "# This is why we only go up to the 7th degree polynomial\n",
    "\n",
    "# Compute RMSE on test set:\n",
    "for ii in range(numRepeats): # Loop from 0 to 99\n",
    "    testIndex = np.random.randint(0,numData,1) # Randomize test index - pick randint from 0 to 7\n",
    "    testSet = samplingIndices[testIndex] # Find the test set (= 1 random data point from our 8)\n",
    "    trainingSet = np.copy([samplingIndices]) # Make copy of sampling indices\n",
    "    trainingSet = np.delete(trainingSet,testIndex) # Delete the test subset\n",
    "    for jj in range(numData-1): # Loop from 0 to 6 - for each poly degree\n",
    "        numDegrees = jj+1 # degrees are from 1 to 7, so add 1 to jj each time\n",
    "        p = np.polyfit(x[trainingSet],y[trainingSet],numDegrees) # compute coefficients\n",
    "        yHat = np.polyval(p,x)  # then evaluate\n",
    "        # Calculate RMSE with the test set (just the single point we randomly chose above):\n",
    "        rmse[ii,jj] = np.sqrt(np.mean((y[testSet] - yHat[testSet])**2)) # store this in rmse container\n",
    "\n",
    "# Plot data:\n",
    "plt.plot(np.linspace(1,numData-1,7),np.mean(rmse,axis=0))\n",
    "plt.title('Real RMSE as a function of degree of polynomial')\n",
    "plt.xlabel('Degree of polynomial')\n",
    "plt.ylabel('RMSE measured only at points left out from building model')   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The solution? Where RMSE is minimal\n",
    "solution = np.amin(np.mean(rmse,axis=0)) # value\n",
    "index = np.argmin(np.mean(rmse,axis=0)) # index\n",
    "print('The RMSE is minimal at polynomial of degree: {}'.format(index+1)) \n",
    "\n",
    "# Note - the console will give you warnings that the polyfit is poorly conditioned sometimes. \n",
    "# That's another dead giveaway that you are overfitting. Too many parameters, not enough data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Sata:\n",
    "n = 200\n",
    "satM = 500 + np.random.normal(0,1,n) * 111    \n",
    "\n",
    "# Compute descriptives:\n",
    "satMean = np.mean(satM)\n",
    "satMedian = np.median(satM)\n",
    "satMin = np.min(satM)\n",
    "satMax = np.max(satM)\n",
    "\n",
    "# Compute correlation coefficient:\n",
    "Y = satM + 200 * np.random.normal(0,1,n)\n",
    "Y = np.round(Y/10)\n",
    "X = np.round(satM)\n",
    "r = np.corrcoef(X,Y)\n",
    "\n",
    "# Do that again on more sata:\n",
    "x = satM/200\n",
    "Y2 = np.log(x) + 0.5 * np.random.normal(0,1,n)\n",
    "r2 = np.corrcoef(x,Y2)\n",
    "\n",
    "# Reformat Y2:\n",
    "Y2 = np.round(Y2*21 + 20)\n",
    "\n",
    "# Run regression:\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression().fit(satM.reshape(len(satM),1),Y2)\n",
    "b0, b1 = model.intercept_, model.coef_\n",
    "yHat = b1 * satM + b0\n",
    "\n",
    "# Plot data:\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(satM,Y2,'o',markersize=3)\n",
    "plt.xlabel('Math SAT score')\n",
    "plt.ylabel('Class grade')\n",
    "plt.plot(satM,yHat,color='orange',linewidth=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy example to show overfitting\n",
    "Q = np.array([[satM[7],Y2[7]],[satM[180],Y2[180]]]) # pick 2 points\n",
    "model = LinearRegression().fit(Q[:,0].reshape(len(Q),1),Q[:,1]) # fit model\n",
    "b0, b1 = model.intercept_, model.coef_ # extract betas\n",
    "yHat = b1 * satM + b0 # build model\n",
    "plt.plot(Q[:,0],Q[:,1],'x',color='red',markersize=5) # plot data\n",
    "plt.xlabel('Math SAT score')\n",
    "plt.ylabel('Class grade')\n",
    "plt.plot(satM,yHat,color='orange',linewidth=0.5) # plot line\n",
    "r_sq = model.score(Q[:,0].reshape(len(Q),1),Q[:,1])\n",
    "print(r_sq) # Captures 100% of the variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving on to data with multiple covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data:\n",
    "x = np.genfromtxt('mRegDataX.csv',delimiter=',') # satM satV hoursS gpa appreciation fearM fearT\n",
    "y = np.genfromtxt('mRegDataY.csv',delimiter=',') # outcome: class score\n",
    "\n",
    "# Doing the full model and calculating the yhats:\n",
    "model = LinearRegression().fit(x,y)\n",
    "b0, b1 = model.intercept_, model.coef_\n",
    "y_hat = np.dot(b1,x.transpose()) + b0\n",
    "\n",
    "# Scatter plot between predicted and actual score of full model:\n",
    "r = np.corrcoef(y_hat,y)\n",
    "plt.plot(y_hat,y,'o',markersize=5)\n",
    "plt.xlabel('Predicted grade score')\n",
    "plt.ylabel('Actual grade score')\n",
    "plt.title('R: {:.3f}'.format(r[0,1])) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data:\n",
    "x = np.genfromtxt('mRegDataX.csv',delimiter=',') # satM satV hoursS gpa appreciation fearM fearT\n",
    "y = np.genfromtxt('mRegDataY.csv',delimiter=',') # outcome: class score\n",
    "\n",
    "# 2. Doing the full model and calculating the yhats:\n",
    "model = LinearRegression().fit(x,y)\n",
    "b0, b1 = model.intercept_, model.coef_\n",
    "y_hat = np.dot(b1,x.transpose()) + b0\n",
    "\n",
    "# 3. Scatter plot between predicted and actual score of full model:\n",
    "r = np.corrcoef(y_hat,y)\n",
    "plt.plot(y_hat,y,'o',markersize=5)\n",
    "plt.xlabel('Predicted grade score')\n",
    "plt.ylabel('Actual grade score')\n",
    "plt.title('R: {:.3f}'.format(r[0,1])) \n",
    "\n",
    "# 4. Splitting the dataset for cross-validation:\n",
    "x1 = np.copy(x[0:100,:])\n",
    "y1 = np.copy(y[0:100])\n",
    "model = LinearRegression().fit(x1,y1)\n",
    "b0_1, b1_1 = model.intercept_, model.coef_\n",
    "\n",
    "x2 = np.copy(x[100:,:])\n",
    "y2 = np.copy(y[100:])\n",
    "model = LinearRegression().fit(x2,y2)\n",
    "b0_2, b1_2 = model.intercept_, model.coef_\n",
    "\n",
    "# 5. Cross-validation. Using the betas from one dataset, but\n",
    "# measuring the error with the other dataset\n",
    "y_hat1 = np.dot(b1_2,x1.transpose()) + b0_2\n",
    "y_hat2 = np.dot(b1_1,x2.transpose()) + b0_1\n",
    "rmse1 = np.sqrt(np.mean((y_hat1 - y1)**2))\n",
    "rmse2 = np.sqrt(np.mean((y_hat2 - y2)**2))\n",
    "print(rmse1, rmse2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using ridge regression to find optimal lambda: a scikit-learn implementation\n",
    "# Load libraries:\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Init parameters:\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x, y.reshape(-1,1), test_size=0.5, random_state=0)\n",
    "lambdas = np.linspace(-20,20,201)\n",
    "cont = np.empty([len(lambdas),2])*np.NaN # [lambda error]\n",
    "for ii in range(len(lambdas)):\n",
    "    ridgeModel = Ridge(alpha=lambdas[ii]).fit(xTrain, yTrain)\n",
    "    cont[ii,0] = lambdas[ii]\n",
    "    error = mean_squared_error(yTest,ridgeModel.predict(xTest),squared=False)\n",
    "    cont[ii,1] = error\n",
    "\n",
    "plt.plot(cont[:,0],cont[:,1])\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Ridge regression')\n",
    "plt.show()\n",
    "print('Optimal lambda:',lambdas[np.argmax(cont[:,1]==np.min(cont[:,1]))])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% 5. Now do the same thing--but with lasso regression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # Just to ignore warnings that might be thrown due to artifically formed data.\n",
    "\n",
    "# Load libraries:\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Init parameters:\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x, y.reshape(-1,1), test_size=0.2, random_state=0)\n",
    "lambdas = np.linspace(-20,20,201)\n",
    "cont = np.empty([len(lambdas),2])*np.NaN # [lambda error]\n",
    "\n",
    "for ii in range(len(lambdas)):\n",
    "    ridgeModel = Lasso(alpha=lambdas[ii]).fit(xTrain, yTrain)\n",
    "    cont[ii,0] = lambdas[ii]\n",
    "    error = mean_squared_error(yTest,ridgeModel.predict(xTest),squared=False)\n",
    "    cont[ii,1] = error\n",
    "\n",
    "plt.plot(cont[:,0],cont[:,1])\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Lasso regression')\n",
    "plt.show()\n",
    "print('Optimal lambda:',lambdas[np.argmax(cont[:,1]==np.min(cont[:,1]))])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
